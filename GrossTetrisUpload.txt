import random
import gymnasium as gym

from tetris_gymnasium.envs.tetris import Tetris
total_actions = 0
agent = "Random no hold"

if __name__ == "__main__":
    env = gym.make("tetris_gymnasium/Tetris", render_mode="ansi")
    observation, info = env.reset(seed=42)
    manual_action_space = [0,1,2,3,4,7]
    print(f"Obervation: {observation}\n")
    print(f"Info: {info}\n")
    print("\n\n\n\n\n\n")
    terminated = False
    while not terminated:
        print(env.render() + "\n")
        #Choose action based on agent
        if(agent == "Random"):
            action = env.action_space.sample()
        if(agent == "Random no hold"):
            action = random.choice(manual_action_space)
            total_actions += 1
        if(agent == "Agent 1"):
            action = agent1_choose(manual_action_space, observation)
        #Print Action
        print("\nAction: ")
        if(action == 0):
            print("Move Left")
        elif(action == 1):
            print("Move Right")
        elif(action == 2):
            print("Move Down")
        elif(action == 3):
            print("Rotate Clockwise")
        elif(action == 4):
            print("Rotate Counterclockwise")
        elif(action == 5):
            print("Hard Drop")
        elif(action == 6):
            print("Swap")
        elif(action == 7):
            print("No-Operation")
        observation, reward, terminated, truncated, info = env.step(action)
    print("Game Over!")
    print(f"Survived {total_actions} actions")



______________________________________________________________________

#This will hold past results
def createHistory():
    return {}

#This will train using random actions (we removed swap and hard drop for simplicity)
def simple_train_random(num_iter, history):
    agent = "Random no hold"

    for i in range(num_iter):
        env = gym.make("tetris_gymnasium/Tetris", render_mode="ansi")
        observation, info = env.reset(seed=42)
        manual_action_space = [0,1,2,3,4,7]
        terminated = False

        #Selects a random choice
        while not terminated:
            action = random.choice(manual_action_space)
            observation, reward, terminated, truncated, info = env.step(action)

            # Gets board 
            if isinstance(observation, dict):
                observation = observation["board"]

            # Convert board to a tuple key
            key = tuple(observation.flatten())

            #Updates the history, gets a reward of 1 for each action survived
            if key in history:
                if action in history[key]:
                    history[key][action][0] += reward
                    history[key][action][1] += 1
                else:
                    history[key][action] = [reward, 1]
            else:
                history[key] = {action: [reward, 1]}

        print(f"Iteration {i+1}, History Size: {len(history)}")  # Debugging output




______________________________________________

history1 = createHistory()


__________________________________________________
#Example of a created historyy

history1 = createHistory()
simple_train_random(10, history1)
print(len(history1))  # Should now output a number > 0


_______________________________________________________
def agent1_choose(action_space, observation, history):
    board = observation["board"]



______________________________________________________________________
import sys
print("Test2")
import cv2
import gymnasium as gym

from tetris_gymnasium.envs import Tetris

if __name__ == "__main__":
    # Create an instance of Tetris
    env = gym.make("tetris_gymnasium/Tetris", render_mode="human")
    env.reset(seed=42)
    print("Test3")

    # Main game loop
    terminated = False
    #cv2.namedWindow("Tetris")
    print("Test 4")

    while not terminated:
        # Render the current state of the game as text
        env.render()

        # Pick an action from user input mapped to the keyboard
        action = None
        while action is None:
            key = cv2.waitKey(1)

            if key == ord("a"):
                action = env.unwrapped.actions.move_left
            elif key == ord("d"):
                action = env.unwrapped.actions.move_right
            elif key == ord("s"):
                action = env.unwrapped.actions.move_down
            elif key == ord("w"):
                action = env.unwrapped.actions.rotate_counterclockwise
            elif key == ord("e"):
                action = env.unwrapped.actions.rotate_clockwise
            elif key == ord(" "):
                action = env.unwrapped.actions.hard_drop
            elif key == ord("q"):
                action = env.unwrapped.actions.swap
            elif key == ord("r"):
                env.reset(seed=42)
                break

            if (
                cv2.getWindowProperty(env.unwrapped.window_name, cv2.WND_PROP_VISIBLE)
                == 0
            ):
                sys.exit()

        # Perform the action
        observation, reward, terminated, truncated, info = env.step(action)

    # Game over
    print("Game Over!")










______________________________________________________________________________
import torch
import torch.nn as nn
import torch.optim as optim
import random
import numpy as np
import gymnasium as gym
from collections import deque

# Define the neural network for Q-learning
class DQN(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(DQN, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, output_dim)
        )

    def forward(self, x):
        return self.network(x)

_____________________________________________________________________________
class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        return np.array(states), actions, rewards, np.array(next_states), dones

    def __len__(self):
        return len(self.buffer)
__________________________________________________________________________
import gymnasium as gym
import torch

env = gym.make("tetris_gymnasium/Tetris", render_mode="ansi")
state_dict, _ = env.reset()  # state is a dictionary
state = state_dict["board"]  # Adjust this to extract the correct feature

def preprocess_state(state):
    return torch.tensor(state, dtype=torch.float32).flatten()

preprocessed_state = preprocess_state(state)  # Now works correctly

input_dim = len(preprocessed_state)  # Use the correct input size
agent = DQNAgent(input_dim, env.action_space.n)  # Now passes correct input_dim
______________________________________________________________________________
class DQNAgent:
    def __init__(self, input_dim, output_dim):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = DQN(input_dim, output_dim).to(self.device)
        self.target_model = DQN(input_dim, output_dim).to(self.device)
        self.target_model.load_state_dict(self.model.state_dict())  # Copy weights

        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-4)
        self.loss_fn = nn.MSELoss()
        self.replay_buffer = ReplayBuffer(10000)
        self.batch_size = 64
        self.gamma = 0.99
        self.epsilon = 1.0  # Initial exploration rate
        self.epsilon_min = 0.05
        self.epsilon_decay = 0.995
        self.update_target_every = 1000  # Steps to update target network
        self.steps = 0

    def select_action(self, state):
        """Epsilon-greedy action selection"""
        if random.random() < self.epsilon:
            return env.action_space.sample()
        else:
            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            with torch.no_grad():
                q_values = self.model(state)
            return torch.argmax(q_values).item()

    def train_step(self):
        """Sample from replay buffer and update the model"""
        if len(self.replay_buffer) < self.batch_size:
            return

        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)

        states = torch.FloatTensor(states).to(self.device)
        actions = torch.LongTensor(actions).unsqueeze(1).to(self.device)
        rewards = torch.FloatTensor(rewards).to(self.device)
        next_states = torch.FloatTensor(next_states).to(self.device)
        dones = torch.FloatTensor(dones).to(self.device)

        q_values = self.model(states).gather(1, actions).squeeze(1)

        with torch.no_grad():
            max_next_q_values = self.target_model(next_states).max(1)[0]
            target_q_values = rewards + (1 - dones) * self.gamma * max_next_q_values

        loss = self.loss_fn(q_values, target_q_values)

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        self.steps += 1
        if self.steps % self.update_target_every == 0:
            self.target_model.load_state_dict(self.model.state_dict())

        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)



____________________________________________________________________________
agent = DQNAgent(input_dim, num_actions)
num_episodes = 5000

for episode in range(num_episodes):
    state_dict, _ = env.reset() 
    state = state_dict["board"]  

    state = preprocess_state(state) 
    terminated = False
    total_reward = 0

    while not terminated:
        action = agent.select_action(state)
        next_state_dict, reward, terminated, truncated, _ = env.step(action)
        next_state = next_state_dict["board"]  
        next_state = preprocess_state(next_state)  

        agent.replay_buffer.push(state, action, reward, next_state, terminated)
        agent.train_step()

        state = next_state
        total_reward += reward

    print(f"Episode {episode + 1}: Total Reward = {total_reward}")

    if (episode + 1) % 100 == 0:
        torch.save(agent.model.state_dict(), f"tetris_dqn_{episode + 1}.pth")



